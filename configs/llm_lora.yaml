# LLM LoRA Fine-tuning Configuration
# Used with: Mistral-7B, LLaMA-3-8B, Qwen2-7B, Gemma-2-9B, Mixtral-8x7B

data_dir: data/processed
max_length: 2048

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
use_4bit: true

# Training
num_epochs: 1
batch_size: 1
gradient_accumulation: 8
learning_rate: 2e-4
warmup_steps: 100
eval_steps: 500
save_steps: 500
max_train_samples: 50000
seed: 42
